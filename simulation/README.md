# Water pipeline simulation
This simple python script simulates a hardcoded water pipeline. Whenever the state changes, updates are sent to the 
_Kafka_ brokers specified.

## Design
The package [confluent-kafka](https://github.com/confluentinc/confluent-kafka-python) is used to communicate with the 
_Kafka_ brokers. 

The simulation is a cyclical process that repeats every **x** seconds. First, the new values for each element are computed.
After that a new batch of messages is created and sent to _Kafka_.

### Pipeline elements
#### Ports
Ports represent interfaces of the pipeline to the outside world, e.g. pumps providing water or connections to factories/households.
They have several parameters:
- `id`: String with the name of the `Port` instance.
- `type`: String with the object class - always `port`.
- `production`: Number of the current amount of water transported by the instance. Note that depending on the use of the 
instance this can either be producing water into or out of the system.
- `desired`: Number with the desired production value. Every update the production value will be updated until it reaches
this value.
- `delta`: Number showing the amount with wich `production` changes per cycle.
- `limit`: Number showing the maximum value for the `production` parameter.

#### Basins

- `id`: String with the name of the `Basin` instance.
- `type`: String with the object class - always `basin`.
- `load`: Number for the current amount of water contained by the instance.
- `limit`: Number showing the maximum value for the `load` parameter.
- `inlets`: Array of `Port` objects that provide water to the `Basin` instance.
- `outlets`: Array of `Port` objects that consume water from the `Basin` instance.

## Message specifications
All messages are sent in JSON format.

### Pipeline updates
All updates are generated by referring to the current pipeline instance as `this` in the following examples. The following
samples are taken verbatim from the script.

#### Ports
```python3
{
    "id": self.id,
    "type": "port",
    "production": self.production,
    "desired": self.desired,
    "delta": self.delta,
    "prod_limit": self.limit,
    "timestamp": str(datetime.now(timezone.utc)),
}
```

#### Basins
```python3
{
    "id": self.id,
    "type": "basin",
    "load": self.load,
    "load_limit": self.limit,
    "inlets": [output.id for output in self.inlets],
    "outlets": [output.id for output in self.outlets],
    "timestamp": str(datetime.now(timezone.utc)),
}
```

## Usage
This image can be built and run inside of _docker_ using the standard methods for doing so. However, we also already 
include it in our top-level directory's `docker-compose.yml`. All environment variables have been set there.

### Environment variables
There are two environment variables that dictate the behavior of the container. These can be set in the `docker-compose.yml`
file.

- `kafka_brokers`: a list of _Kafka_ brokers to communicate with. Each broker consists of an either an IP address or container
name, followed by the port listened to. Brokers are separated by a `;`. Example: `kafka_brokers=kafka_0:29091;kafka_1:29092`. 
- `kafka_topic`: a single string with the _Kafka_ topic communicated on. Example:
`kafka_topic=simulation_updates`. Note, this value must obviously be the same as the topic used by the data consumers.